{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch transformers datasets rouge-score nltk tqdm pandas numpy sentencepiece protobuf accelerate evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-29T04:31:23.387399Z","iopub.execute_input":"2025-10-29T04:31:23.387707Z","iopub.status.idle":"2025-10-29T04:32:45.244703Z","shell.execute_reply.started":"2025-10-29T04:31:23.387684Z","shell.execute_reply":"2025-10-29T04:32:45.243893Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verify installation","metadata":{}},{"cell_type":"code","source":"import torch\nimport transformers\nimport datasets\nfrom rouge_score import rouge_scorer\nimport nltk\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Transformers version: {transformers.__version__}\")\nprint(f\"Datasets version: {datasets.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T04:32:57.429047Z","iopub.execute_input":"2025-10-29T04:32:57.429835Z","iopub.status.idle":"2025-10-29T04:32:58.856299Z","shell.execute_reply.started":"2025-10-29T04:32:57.429804Z","shell.execute_reply":"2025-10-29T04:32:58.855481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## View dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load the datasets\nprint(\"=\" * 80)\nprint(\"LOADING CNN/DAILYMAIL DATASET\")\nprint(\"=\" * 80)\n\n# Load train, validation, and test datasets\ntrain_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\nvalidation_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')\ntest_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DATASET OVERVIEW\")\nprint(\"=\" * 80)\nprint(f\"Train set size: {len(train_df):,} samples\")\nprint(f\"Validation set size: {len(validation_df):,} samples\")\nprint(f\"Test set size: {len(test_df):,} samples\")\nprint(f\"Total samples: {len(train_df) + len(validation_df) + len(test_df):,}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DATASET COLUMNS\")\nprint(\"=\" * 80)\nprint(f\"Column names: {list(train_df.columns)}\")\nprint(f\"Column types:\\n{train_df.dtypes}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BASIC STATISTICS\")\nprint(\"=\" * 80)\nprint(train_df.info())\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MISSING VALUES CHECK\")\nprint(\"=\" * 80)\nprint(f\"Train set missing values:\\n{train_df.isnull().sum()}\")\nprint(f\"\\nValidation set missing values:\\n{validation_df.isnull().sum()}\")\nprint(f\"\\nTest set missing values:\\n{test_df.isnull().sum()}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FIRST 5 ENTRIES FROM TRAINING SET\")\nprint(\"=\" * 80)\nprint(train_df.head())\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DETAILED VIEW OF FIRST 5 ENTRIES\")\nprint(\"=\" * 80)\n\nfor idx in range(min(5, len(train_df))):\n    print(f\"\\n{'─' * 80}\")\n    print(f\"SAMPLE {idx + 1}\")\n    print(f\"{'─' * 80}\")\n    \n    row = train_df.iloc[idx]\n    \n    for col in train_df.columns:\n        print(f\"\\n[{col.upper()}]:\")\n        content = str(row[col])\n        # Truncate long text for better readability\n        if len(content) > 500:\n            print(content[:500] + f\"... (truncated, total length: {len(content)} chars)\")\n        else:\n            print(content)\n    \n    print(f\"\\n{'─' * 80}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TEXT LENGTH STATISTICS\")\nprint(\"=\" * 80)\n\n# Calculate text lengths\ntrain_df['article_length'] = train_df.iloc[:, 0].astype(str).apply(len)\ntrain_df['summary_length'] = train_df.iloc[:, 1].astype(str).apply(len) if len(train_df.columns) > 1 else 0\n\nprint(\"\\nArticle Length Statistics:\")\nprint(train_df['article_length'].describe())\n\nif len(train_df.columns) > 1:\n    print(\"\\nSummary Length Statistics:\")\n    print(train_df['summary_length'].describe())\n    \n    print(f\"\\nAverage Compression Ratio: {train_df['article_length'].mean() / train_df['summary_length'].mean():.2f}x\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EXPLORATION COMPLETE\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T04:33:02.361007Z","iopub.execute_input":"2025-10-29T04:33:02.361884Z","iopub.status.idle":"2025-10-29T04:33:48.495663Z","shell.execute_reply.started":"2025-10-29T04:33:02.361861Z","shell.execute_reply":"2025-10-29T04:33:48.494999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T04:35:55.996878Z","iopub.execute_input":"2025-10-29T04:35:55.997273Z","iopub.status.idle":"2025-10-29T04:36:08.849372Z","shell.execute_reply.started":"2025-10-29T04:35:55.997247Z","shell.execute_reply":"2025-10-29T04:36:08.848604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nKAGGLE SETUP - RUN THIS CELL FIRST\nThis will fix the transformers library compatibility issues\n\"\"\"\n\nprint(\"=\"*80)\nprint(\"SETTING UP ENVIRONMENT FOR T5 SUMMARIZATION\")\nprint(\"=\"*80)\n\nprint(\"\\nUpgrading transformers library (this may take 1-2 minutes)...\")\n\n# Upgrade transformers and dependencies\n!pip install --upgrade --quiet transformers==4.35.0 huggingface_hub tokenizers\n\nprint(\"\\n✓ Libraries updated successfully!\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"⚠️ IMPORTANT: RESTART KERNEL NOW!\")\nprint(\"=\"*80)\nprint(\"\\nSteps:\")\nprint(\"1. Click 'Kernel' → 'Restart Kernel' in the top menu\")\nprint(\"2. After restart, run the preprocessing script\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T04:44:47.766903Z","iopub.execute_input":"2025-10-29T04:44:47.767249Z","iopub.status.idle":"2025-10-29T04:44:57.429896Z","shell.execute_reply.started":"2025-10-29T04:44:47.767224Z","shell.execute_reply":"2025-10-29T04:44:57.428977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test processing","metadata":{}},{"cell_type":"code","source":"\"\"\"\nCNN/DailyMail Preprocessing Script for T5 Summarization\nStep 1: Data Preprocessing - FIXED FOR KAGGLE\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer  # Using AutoTokenizer instead of T5Tokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"STEP 1: PREPROCESSING SCRIPT FOR T5 SUMMARIZATION\")\nprint(\"=\"*80)\n\n# ============================================================================\n# 1. CONFIGURATION\n# ============================================================================\nclass Config:\n    MODEL_NAME = \"t5-small\"  # or \"t5-base\" for better quality\n    MAX_SOURCE_LENGTH = 512  # Maximum length of input article\n    MAX_TARGET_LENGTH = 128  # Maximum length of output summary\n    BATCH_SIZE = 8           # Adjust based on your GPU memory\n    NUM_SAMPLES = 1000       # Use subset for faster testing (remove for full training)\n    \nconfig = Config()\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  Model: {config.MODEL_NAME}\")\nprint(f\"  Max source length: {config.MAX_SOURCE_LENGTH}\")\nprint(f\"  Max target length: {config.MAX_TARGET_LENGTH}\")\nprint(f\"  Batch size: {config.BATCH_SIZE}\")\n\n# ============================================================================\n# 2. LOAD TOKENIZER - FIXED VERSION\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Loading T5 Tokenizer...\")\nprint(f\"{'='*80}\")\n\n# Use AutoTokenizer which avoids the chat template issue\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\nprint(f\"✓ Tokenizer loaded successfully\")\nprint(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n\n# ============================================================================\n# 3. TEXT CLEANING FUNCTIONS\n# ============================================================================\ndef clean_text(text):\n    \"\"\"Clean and normalize text\"\"\"\n    # Convert to string and strip whitespace\n    text = str(text).strip()\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove special characters that might cause issues\n    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n    \n    return text\n\ndef preprocess_text(article, summary):\n    \"\"\"Preprocess article and summary\"\"\"\n    # Clean texts\n    article = clean_text(article)\n    summary = clean_text(summary)\n    \n    # T5 requires \"task prefix\" before input\n    # For summarization, we add \"summarize: \" prefix\n    article = \"summarize: \" + article\n    \n    return article, summary\n\n# ============================================================================\n# 4. LOAD DATASETS\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Loading CNN/DailyMail Datasets...\")\nprint(f\"{'='*80}\")\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\nval_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')\ntest_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')\n\nprint(f\"✓ Original sizes:\")\nprint(f\"  Train: {len(train_df):,}\")\nprint(f\"  Validation: {len(val_df):,}\")\nprint(f\"  Test: {len(test_df):,}\")\n\n# Use subset for testing (remove these lines for full training)\nif config.NUM_SAMPLES:\n    train_df = train_df.head(config.NUM_SAMPLES)\n    val_df = val_df.head(config.NUM_SAMPLES // 10)\n    test_df = test_df.head(config.NUM_SAMPLES // 10)\n    print(f\"\\n⚠ Using subset for testing:\")\n    print(f\"  Train: {len(train_df):,}\")\n    print(f\"  Validation: {len(val_df):,}\")\n    print(f\"  Test: {len(test_df):,}\")\n\n# ============================================================================\n# 5. CUSTOM DATASET CLASS\n# ============================================================================\nclass SummarizationDataset(Dataset):\n    \"\"\"Custom Dataset for T5 Summarization\"\"\"\n    \n    def __init__(self, dataframe, tokenizer, max_source_length, max_target_length):\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_source_length = max_source_length\n        self.max_target_length = max_target_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Get article and summary\n        article = row['article']\n        summary = row['highlights']\n        \n        # Preprocess\n        article, summary = preprocess_text(article, summary)\n        \n        # Tokenize source (article)\n        source_encoding = self.tokenizer(\n            article,\n            max_length=self.max_source_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Tokenize target (summary)\n        target_encoding = self.tokenizer(\n            summary,\n            max_length=self.max_target_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Prepare labels (replace padding token id with -100)\n        labels = target_encoding['input_ids'].clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        return {\n            'input_ids': source_encoding['input_ids'].squeeze(),\n            'attention_mask': source_encoding['attention_mask'].squeeze(),\n            'labels': labels.squeeze()\n        }\n\n# ============================================================================\n# 6. CREATE DATASETS\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Creating PyTorch Datasets...\")\nprint(f\"{'='*80}\")\n\ntrain_dataset = SummarizationDataset(\n    train_df, \n    tokenizer, \n    config.MAX_SOURCE_LENGTH, \n    config.MAX_TARGET_LENGTH\n)\n\nval_dataset = SummarizationDataset(\n    val_df, \n    tokenizer, \n    config.MAX_SOURCE_LENGTH, \n    config.MAX_TARGET_LENGTH\n)\n\ntest_dataset = SummarizationDataset(\n    test_df, \n    tokenizer, \n    config.MAX_SOURCE_LENGTH, \n    config.MAX_TARGET_LENGTH\n)\n\nprint(f\"✓ Datasets created:\")\nprint(f\"  Train dataset size: {len(train_dataset):,}\")\nprint(f\"  Validation dataset size: {len(val_dataset):,}\")\nprint(f\"  Test dataset size: {len(test_dataset):,}\")\n\n# ============================================================================\n# 7. CREATE DATALOADERS\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Creating DataLoaders...\")\nprint(f\"{'='*80}\")\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config.BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config.BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=config.BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"✓ DataLoaders created:\")\nprint(f\"  Train batches: {len(train_loader):,}\")\nprint(f\"  Validation batches: {len(val_loader):,}\")\nprint(f\"  Test batches: {len(test_loader):,}\")\n\n# ============================================================================\n# 8. VERIFY PREPROCESSING\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Verifying Preprocessing - Sample Batch\")\nprint(f\"{'='*80}\")\n\n# Get one batch\nsample_batch = next(iter(train_loader))\n\nprint(f\"\\nBatch shapes:\")\nprint(f\"  Input IDs: {sample_batch['input_ids'].shape}\")\nprint(f\"  Attention Mask: {sample_batch['attention_mask'].shape}\")\nprint(f\"  Labels: {sample_batch['labels'].shape}\")\n\nprint(f\"\\nDecoding first sample:\")\nprint(f\"\\n{'─'*80}\")\nprint(\"INPUT (Article):\")\nprint(f\"{'─'*80}\")\ndecoded_input = tokenizer.decode(sample_batch['input_ids'][0], skip_special_tokens=True)\nprint(decoded_input[:500] + \"...\" if len(decoded_input) > 500 else decoded_input)\n\nprint(f\"\\n{'─'*80}\")\nprint(\"TARGET (Summary):\")\nprint(f\"{'─'*80}\")\n# Replace -100 with pad_token_id for decoding\nlabels_for_decode = sample_batch['labels'][0].clone()\nlabels_for_decode[labels_for_decode == -100] = tokenizer.pad_token_id\ndecoded_target = tokenizer.decode(labels_for_decode, skip_special_tokens=True)\nprint(decoded_target)\n\n# ============================================================================\n# 9. PREPROCESSING STATISTICS\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Preprocessing Statistics\")\nprint(f\"{'='*80}\")\n\n# Calculate actual lengths\nsample_article = train_df.iloc[0]['article']\nsample_summary = train_df.iloc[0]['highlights']\n\npreprocessed_article, preprocessed_summary = preprocess_text(sample_article, sample_summary)\ntokenized_article = tokenizer(preprocessed_article, truncation=False)\ntokenized_summary = tokenizer(preprocessed_summary, truncation=False)\n\nprint(f\"\\nSample text lengths:\")\nprint(f\"  Original article chars: {len(sample_article):,}\")\nprint(f\"  Preprocessed article tokens: {len(tokenized_article['input_ids'])}\")\nprint(f\"  Original summary chars: {len(sample_summary):,}\")\nprint(f\"  Preprocessed summary tokens: {len(tokenized_summary['input_ids'])}\")\nprint(f\"  Will truncate articles longer than: {config.MAX_SOURCE_LENGTH} tokens\")\nprint(f\"  Will truncate summaries longer than: {config.MAX_TARGET_LENGTH} tokens\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ PREPROCESSING COMPLETE!\")\nprint(f\"{'='*80}\")\nprint(\"\\nData is ready for fine-tuning!\")\nprint(\"Next step: Fine-tuning the T5 model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T04:46:09.215393Z","iopub.execute_input":"2025-10-29T04:46:09.216221Z","iopub.status.idle":"2025-10-29T04:46:09.871743Z","shell.execute_reply.started":"2025-10-29T04:46:09.216183Z","shell.execute_reply":"2025-10-29T04:46:09.870713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## T5 Finetunning","metadata":{}},{"cell_type":"code","source":"\"\"\"\nCNN/DailyMail Fine-tuning Script for T5 Summarization\nStep 2: Model Fine-tuning\n\"\"\"\n\nimport torch\nfrom transformers import T5ForConditionalGeneration, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW  # Import AdamW from PyTorch instead\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport time\nimport os\n\nprint(\"=\"*80)\nprint(\"STEP 2: FINE-TUNING T5 MODEL\")\nprint(\"=\"*80)\n\n# ============================================================================\n# 1. TRAINING CONFIGURATION\n# ============================================================================\nclass TrainingConfig:\n    # Model\n    MODEL_NAME = \"t5-small\"\n    \n    # Training parameters\n    EPOCHS = 3                    # Number of training epochs\n    LEARNING_RATE = 3e-4          # Learning rate\n    WARMUP_STEPS = 500            # Warmup steps for scheduler\n    GRADIENT_ACCUMULATION = 4     # Accumulate gradients (effective batch = 8*4=32)\n    MAX_GRAD_NORM = 1.0           # Gradient clipping\n    \n    # Paths\n    OUTPUT_DIR = \"/kaggle/working/t5_summarization\"\n    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n    \n    # Device\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \ntraining_config = TrainingConfig()\n\n# Create directories\nos.makedirs(training_config.OUTPUT_DIR, exist_ok=True)\nos.makedirs(training_config.CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"\\nTraining Configuration:\")\nprint(f\"  Model: {training_config.MODEL_NAME}\")\nprint(f\"  Device: {training_config.DEVICE}\")\nprint(f\"  Epochs: {training_config.EPOCHS}\")\nprint(f\"  Learning Rate: {training_config.LEARNING_RATE}\")\nprint(f\"  Gradient Accumulation Steps: {training_config.GRADIENT_ACCUMULATION}\")\nprint(f\"  Output Directory: {training_config.OUTPUT_DIR}\")\n\n# Check CUDA\nif torch.cuda.is_available():\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nelse:\n    print(\"  ⚠ Running on CPU - Training will be slower!\")\n\n# ============================================================================\n# 2. LOAD PRE-TRAINED MODEL\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Loading Pre-trained T5 Model...\")\nprint(f\"{'='*80}\")\n\nmodel = T5ForConditionalGeneration.from_pretrained(training_config.MODEL_NAME)\nmodel.to(training_config.DEVICE)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"✓ Model loaded successfully\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\n\n# ============================================================================\n# 3. SETUP OPTIMIZER AND SCHEDULER\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Setting up Optimizer and Scheduler...\")\nprint(f\"{'='*80}\")\n\n# Optimizer\noptimizer = AdamW(\n    model.parameters(),\n    lr=training_config.LEARNING_RATE,\n    eps=1e-8\n)\n\n# Calculate total training steps\ntotal_steps = len(train_loader) * training_config.EPOCHS // training_config.GRADIENT_ACCUMULATION\n\n# Learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=training_config.WARMUP_STEPS,\n    num_training_steps=total_steps\n)\n\nprint(f\"✓ Optimizer and scheduler configured\")\nprint(f\"  Total training steps: {total_steps:,}\")\nprint(f\"  Warmup steps: {training_config.WARMUP_STEPS}\")\n\n# ============================================================================\n# 4. TRAINING FUNCTIONS\n# ============================================================================\n\ndef train_epoch(model, dataloader, optimizer, scheduler, device, epoch, gradient_accumulation):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n    \n    optimizer.zero_grad()\n    \n    for step, batch in enumerate(progress_bar):\n        # Move batch to device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss / gradient_accumulation\n        total_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights every gradient_accumulation steps\n        if (step + 1) % gradient_accumulation == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), training_config.MAX_GRAD_NORM)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': f'{loss.item() * gradient_accumulation:.4f}',\n            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n        })\n    \n    avg_loss = total_loss / len(dataloader)\n    return avg_loss\n\ndef validate(model, dataloader, device):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            # Move batch to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            total_loss += outputs.loss.item()\n    \n    avg_loss = total_loss / len(dataloader)\n    return avg_loss\n\n# ============================================================================\n# 5. TRAINING LOOP\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Starting Training...\")\nprint(f\"{'='*80}\")\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'epochs': []\n}\n\nbest_val_loss = float('inf')\nstart_time = time.time()\n\nfor epoch in range(1, training_config.EPOCHS + 1):\n    print(f\"\\n{'─'*80}\")\n    print(f\"Epoch {epoch}/{training_config.EPOCHS}\")\n    print(f\"{'─'*80}\")\n    \n    # Train\n    train_loss = train_epoch(\n        model, \n        train_loader, \n        optimizer, \n        scheduler, \n        training_config.DEVICE,\n        epoch,\n        training_config.GRADIENT_ACCUMULATION\n    )\n    \n    # Validate\n    val_loss = validate(model, val_loader, training_config.DEVICE)\n    \n    # Update history\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['epochs'].append(epoch)\n    \n    # Print epoch summary\n    print(f\"\\nEpoch {epoch} Summary:\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_path = os.path.join(training_config.OUTPUT_DIR, \"best_model\")\n        model.save_pretrained(best_model_path)\n        tokenizer.save_pretrained(best_model_path)\n        print(f\"  ✓ Best model saved! (Val Loss: {val_loss:.4f})\")\n    \n    # Save checkpoint\n    checkpoint_path = os.path.join(training_config.CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch}\")\n    model.save_pretrained(checkpoint_path)\n    tokenizer.save_pretrained(checkpoint_path)\n    print(f\"  ✓ Checkpoint saved: {checkpoint_path}\")\n\n# ============================================================================\n# 6. TRAINING COMPLETE\n# ============================================================================\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ TRAINING COMPLETE!\")\nprint(f\"{'='*80}\")\nprint(f\"\\nTraining Summary:\")\nprint(f\"  Total time: {training_time/60:.2f} minutes ({training_time/3600:.2f} hours)\")\nprint(f\"  Best validation loss: {best_val_loss:.4f}\")\nprint(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\nprint(f\"  Final val loss: {history['val_loss'][-1]:.4f}\")\n\n# ============================================================================\n# 7. PLOT TRAINING CURVES\n# ============================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"Training Loss Curves\")\nprint(f\"{'='*80}\")\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(history['epochs'], history['train_loss'], label='Train Loss', marker='o')\nplt.plot(history['epochs'], history['val_loss'], label='Validation Loss', marker='s')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('T5 Training Progress')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(training_config.OUTPUT_DIR, 'training_curves.png'), dpi=300)\nprint(\"✓ Training curves saved!\")\nplt.show()\n\n# ============================================================================\n# 8. SAVE TRAINING HISTORY\n# ============================================================================\nimport json\n\nhistory_file = os.path.join(training_config.OUTPUT_DIR, 'training_history.json')\nwith open(history_file, 'w') as f:\n    json.dump(history, f, indent=4)\nprint(f\"✓ Training history saved: {history_file}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"Model saved at:\")\nprint(f\"  Best Model: {os.path.join(training_config.OUTPUT_DIR, 'best_model')}\")\nprint(f\"  Checkpoints: {training_config.CHECKPOINT_DIR}\")\nprint(f\"{'='*80}\")\nprint(\"\\nNext step: Evaluation using ROUGE metrics\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}